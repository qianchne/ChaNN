{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### chan 2020/11/19\n",
    "### An simple demo for regression task, using a single layer neural network built by numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "'''\n",
    "x: R^(1,N)\n",
    "W: R^(N,M)\n",
    "b: R^(1,M)\n",
    "y_pridict : R^(1,M)\n",
    "y_true: R^(1,M)\n",
    "'''\n",
    "\n",
    "def forward(W, x, b):\n",
    "    '''\n",
    "    W: weights\n",
    "    x: input vector\n",
    "    b: bias\n",
    "    '''\n",
    "    return np.dot(W, x)+b\n",
    "\n",
    "def backward(L, W, x, b):\n",
    "    '''\n",
    "    input args:\n",
    "    L: y_predict - y_true\n",
    "    delta: here, delta = L, because no activation and only one layer. L层的误差项\n",
    "    W: weights\n",
    "    x: input vector\n",
    "    b: bias\n",
    "    \n",
    "    output args:\n",
    "    dW: dL/dW, graident of W\n",
    "    db: dL/db, gradient of b\n",
    "    '''\n",
    "    N = x.shape[0]\n",
    "    dw = np.dot(L, x.T) # dw: R_(M,N)\n",
    "    db = np.sum(L, axis=1).reshape(N, 1) # db: R_(M, 1) \n",
    "    return dw/N, db/N, L-1\n",
    "\n",
    "def loss_MSE(y_predict, y_true):\n",
    "    return np.mean(np.sum(np.square(y_predict-y_true),axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 3]\n",
      " [6 6]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = [[1,1],[2,2]]\n",
    "b = [[1,1],[2,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9 2 1 ... 3 9 6]\n",
      " [9 6 2 ... 0 1 1]\n",
      " [6 1 4 ... 4 1 5]\n",
      " [9 2 6 ... 4 5 3]\n",
      " [8 3 9 ... 1 7 7]]\n",
      "[[130  44  96 ...  38  77  78]\n",
      " [131  45  97 ...  39  78  79]\n",
      " [166  56  88 ...  58  83  82]\n",
      " [270  65 136 ... 120 163 152]\n",
      " [209  73 147 ...  59 139 118]]\n"
     ]
    }
   ],
   "source": [
    "def dataGen():\n",
    "    '''\n",
    "    regression: y = f(x) = Wx+b\n",
    "    '''\n",
    "    W = np.array([[1, 2, 3, 4, 6],\n",
    "                        [1, 2, 3, 4, 6],\n",
    "                        [3, 4, 5, 6, 2],\n",
    "                        [9, 1, 10, 12, 1],\n",
    "                        [3, 3, 1, 8, 9]])\n",
    "    #x = np.random.randn(1, 10, 2000).reshape(400, 5)\n",
    "    x = np.random.randint(0, 10, 2000).reshape(5, 400)\n",
    "    print(x)\n",
    "    b = np.array([1,2,3,4,5])\n",
    "    y_true = (np.dot(W, x).T+b).T\n",
    "    return x, y_true\n",
    "x, y_true = dataGen()\n",
    "print(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1     iteration:  0 / 200\n",
      "W:  [[-0.14074047 -0.80563766  0.13073955 -0.15350077 -1.7625759 ]\n",
      " [ 0.03332024 -1.80750989  0.10031425 -0.58620485 -1.39013077]\n",
      " [-0.75539723 -0.35620864 -1.45771271 -0.58777818  0.19190497]\n",
      " [ 0.37449362 -0.50447977 -0.1063677  -0.53437406 -0.29044456]\n",
      " [ 0.28822506  0.7472894  -1.56256175 -1.70886817  0.13820128]]\n",
      "b:  [[ 1.50142558]\n",
      " [ 0.03000104]\n",
      " [-1.35147429]\n",
      " [ 0.4128486 ]\n",
      " [-1.11165857]]\n",
      "Loss:  42597.70355008725\n",
      "epoch:  2     iteration:  0 / 200\n",
      "W:  [[ 0.95697783  1.96541285  2.95935678  3.95127454  5.9316655 ]\n",
      " [ 1.04884251  2.04682835  3.04657528  4.05179756  6.07481759]\n",
      " [ 3.13416484  4.11980078  5.12121795  6.14124753  2.2074443 ]\n",
      " [ 9.09058607  1.0884316  10.07822849 12.0838623   1.13828174]\n",
      " [ 3.19642136  3.18752794  1.19112857  8.20556996  9.30379115]]\n",
      "b:  [[ 2.06963225]\n",
      " [ 0.79042788]\n",
      " [-0.26364734]\n",
      " [ 1.85551299]\n",
      " [ 0.11563791]]\n",
      "Loss:  2.221242136774297\n",
      "epoch:  3     iteration:  0 / 200\n",
      "W:  [[ 0.96068683  1.95960952  2.94995912  3.96129099  5.9581917 ]\n",
      " [ 1.04445402  2.04569001  3.05659765  4.0437633   6.04728057]\n",
      " [ 3.11995612  4.123268    5.15269943  6.11808527  2.12757711]\n",
      " [ 9.07882611  1.08101831 10.10034676 12.0775676   1.08383968]\n",
      " [ 3.17950716  3.18449854  1.2285467   8.17671411  9.19092154]]\n",
      "b:  [[ 2.00864773]\n",
      " [ 0.85931985]\n",
      " [-0.07772769]\n",
      " [ 1.97759838]\n",
      " [ 0.39388975]]\n",
      "Loss:  2.436148937287752\n",
      "epoch:  4     iteration:  0 / 200\n",
      "W:  [[ 0.96000264  1.9762083   2.94806558  3.97391665  5.96272466]\n",
      " [ 1.04523302  2.02690605  3.05873266  4.02949764  6.04215469]\n",
      " [ 3.12204557  4.0725966   5.15846962  6.07958911  2.11373975]\n",
      " [ 9.08019722  1.04770389 10.10413179 12.05229864  1.07473939]\n",
      " [ 3.18265268  3.10864769  1.23716476  8.11911258  9.17022224]]\n",
      "b:  [[1.95085136]\n",
      " [0.92468187]\n",
      " [0.09862858]\n",
      " [2.09348341]\n",
      " [0.65782408]]\n",
      "Loss:  1.9645689850701562\n",
      "epoch:  5     iteration:  0 / 200\n",
      "W:  [[ 0.96121756  1.97339738  2.97021411  3.95848422  5.95628147]\n",
      " [ 1.04385907  2.03008491  3.03368488  4.04695021  6.0494413 ]\n",
      " [ 3.11833842  4.08117365  5.0908869   6.12667879  2.13340013]\n",
      " [ 9.07776121  1.05333992 10.05972258 12.08324175  1.08765839]\n",
      " [ 3.17710461  3.12148402  1.13602082  8.18958676  9.19964587]]\n",
      "b:  [[1.89552412]\n",
      " [0.98725148]\n",
      " [0.26745086]\n",
      " [2.20441801]\n",
      " [0.91048254]]\n",
      "Loss:  0.8201032609112202\n",
      "epoch:  6     iteration:  0 / 200\n",
      "W:  [[ 0.96450486  1.96540239  2.96570773  3.96022723  5.96312518]\n",
      " [ 1.04014146  2.03912645  3.03878114  4.04497904  6.04170174]\n",
      " [ 3.10830776  4.1055691   5.1046374   6.12136027  2.11251762]\n",
      " [ 9.07116998  1.06937038 10.06875815 12.0797469   1.07393632]\n",
      " [ 3.16209277  3.15799412  1.15659973  8.18162709  9.16839324]]\n",
      "b:  [[1.84243065]\n",
      " [1.0472949 ]\n",
      " [0.42945712]\n",
      " [2.31087374]\n",
      " [1.15294018]]\n",
      "Loss:  0.5238738461625442\n",
      "epoch:  7     iteration:  0 / 200\n",
      "W:  [[ 0.96890604  1.96961563  2.95389302  3.96345702  5.95540436]\n",
      " [ 1.03516417  2.03436169  3.0521424   4.04132647  6.05043323]\n",
      " [ 3.09487826  4.09271305  5.14068812  6.11150508  2.13607649]\n",
      " [ 9.06234533  1.06092256 10.09244739 12.07327096  1.08941706]\n",
      " [ 3.14199426  3.13875382  1.21055303  8.16687786  9.2036513 ]]\n",
      "b:  [[1.79305737]\n",
      " [1.10313115]\n",
      " [0.5801118 ]\n",
      " [2.40987025]\n",
      " [1.37840912]]\n",
      "Loss:  0.2898855223540241\n",
      "epoch:  8     iteration:  0 / 200\n",
      "W:  [[ 0.9711211   1.96662481  2.98283272  3.97823863  5.9695742 ]\n",
      " [ 1.03265916  2.03774402  3.01941448  4.02460994  6.03440855]\n",
      " [ 3.08811936  4.10183909  5.05238322  6.06640135  2.09283948]\n",
      " [ 9.05790401  1.06691936 10.03442147 12.04363291  1.06100563]\n",
      " [ 3.13187894  3.1524118   1.07839642  8.09937588  9.13894303]]\n",
      "b:  [[1.75041126]\n",
      " [1.15135966]\n",
      " [0.71023963]\n",
      " [2.49537839]\n",
      " [1.57315768]]\n",
      "Loss:  1.613514642488574\n",
      "epoch:  9     iteration:  0 / 200\n",
      "W:  [[ 0.97675279  1.97111454  2.96996503  3.96423992  5.95976509]\n",
      " [ 1.02629028  2.03266657  3.03396656  4.04044108  6.04550167]\n",
      " [ 3.07093515  4.08813937  5.09164693  6.10911618  2.1227704 ]\n",
      " [ 9.04661211  1.05791715 10.060222   12.0717012   1.08067351]\n",
      " [ 3.10616114  3.13190887  1.13715828  8.16330265  9.18373749]]\n",
      "b:  [[1.7064562 ]\n",
      " [1.20106845]\n",
      " [0.84436149]\n",
      " [2.58351104]\n",
      " [1.77388369]]\n",
      "Loss:  0.32791547003965993\n",
      "epoch:  10     iteration:  0 / 200\n",
      "W:  [[ 0.9632125   1.97618819  2.97016306  3.97096063  5.96195097]\n",
      " [ 1.041603    2.02692878  3.0337426   4.03284064  6.04302966]\n",
      " [ 3.1122512   4.07265793  5.09104266  6.08860902  2.11610054]\n",
      " [ 9.07376125  1.04774416 10.05982493 12.05822576  1.07629069]\n",
      " [ 3.1679945   3.10873945  1.13625392  8.13261175  9.17375541]]\n",
      "b:  [[1.66423935]\n",
      " [1.2488115 ]\n",
      " [0.97317948]\n",
      " [2.66815847]\n",
      " [1.96667196]]\n",
      "Loss:  0.6629306929854429\n",
      "epoch:  11     iteration:  0 / 200\n",
      "W:  [[ 0.97409719  1.97851178  2.9781912   3.9793732   5.96963004]\n",
      " [ 1.0292935   2.02430104  3.02466358  4.02332686  6.03434539]\n",
      " [ 3.0790383   4.06556788  5.06654609  6.0629394   2.09266908]\n",
      " [ 9.05193676  1.04308523 10.04372802 12.04135803  1.06089367]\n",
      " [ 3.11828827  3.09812852  1.09959249  8.09419474  9.13868802]]\n",
      "b:  [[1.62732072]\n",
      " [1.29056279]\n",
      " [1.0858308 ]\n",
      " [2.74218264]\n",
      " [2.13526528]]\n",
      "Loss:  0.760544846576816\n",
      "epoch:  12     iteration:  0 / 200\n",
      "W:  [[ 0.97244016  1.98261189  2.98396182  3.98104341  5.97511418]\n",
      " [ 1.03116743  2.01966422  3.01813758  4.02143801  6.02814338]\n",
      " [ 3.08409445  4.05305705  5.04893795  6.05784301  2.07593512]\n",
      " [ 9.0552592   1.03486425 10.03215755 12.03800915  1.04989763]\n",
      " [ 3.12585527  3.07940488  1.07324025  8.08656752  9.11364407]]\n",
      "b:  [[1.59177249]\n",
      " [1.3307643 ]\n",
      " [1.19430056]\n",
      " [2.81345907]\n",
      " [2.29760052]]\n",
      "Loss:  0.1347790386824788\n",
      "epoch:  13     iteration:  0 / 200\n",
      "W:  [[ 0.971947    1.98051696  2.98023813  3.97525074  5.97493008]\n",
      " [ 1.03172515  2.02203338  3.0223487   4.02798894  6.02835158]\n",
      " [ 3.08559926  4.05944941  5.06030019  6.07551841  2.07649687]\n",
      " [ 9.05624803  1.03906473 10.03962379 12.04962381  1.05026676]\n",
      " [ 3.12810737  3.08897164  1.09024493  8.11302043  9.11448478]]\n",
      "b:  [[1.55689296]\n",
      " [1.37020956]\n",
      " [1.30072988]\n",
      " [2.88339471]\n",
      " [2.45688202]]\n",
      "Loss:  0.4131799127731096\n",
      "epoch:  14     iteration:  0 / 200\n",
      "W:  [[ 0.97173704  1.98110843  2.97821871  3.98120091  5.97228318]\n",
      " [ 1.03196259  2.02136449  3.02463247  4.02125989  6.03134497]\n",
      " [ 3.08623992  4.05764463  5.06646214  6.05736242  2.08457347]\n",
      " [ 9.05666901  1.03787879 10.04367286 12.03769335  1.05557397]\n",
      " [ 3.12906617  3.08627062  1.09946686  8.08584827  9.12657217]]\n",
      "b:  [[1.52431942]\n",
      " [1.40704699]\n",
      " [1.40012283]\n",
      " [2.94870669]\n",
      " [2.60563296]]\n",
      "Loss:  0.4422475841073467\n",
      "epoch:  15     iteration:  0 / 200\n",
      "W:  [[ 0.97825803  1.98244234  2.98175202  3.97218855  5.97106566]\n",
      " [ 1.024588    2.01985596  3.02063664  4.03145198  6.03272186]\n",
      " [ 3.06634216  4.0535744   5.05568079  6.08486221  2.08828853]\n",
      " [ 9.04359402  1.03520421 10.03658834 12.0557637   1.05801517]\n",
      " [ 3.0992873   3.08017914  1.08333156  8.1270043   9.13213212]]\n",
      "b:  [[1.49344156]\n",
      " [1.44196678]\n",
      " [1.49434172]\n",
      " [3.01061874]\n",
      " [2.7466404 ]]\n",
      "Loss:  0.2260371519019988\n",
      "epoch:  16     iteration:  0 / 200\n",
      "W:  [[ 0.98179134  1.98069283  2.97843126  3.98086523  5.98444478]\n",
      " [ 1.02059218  2.02183449  3.0243921   4.02163952  6.0175914 ]\n",
      " [ 3.05556081  4.05891276  5.06581358  6.05838671  2.04746426]\n",
      " [ 9.0365095   1.03871209 10.04324668 12.03836642  1.03118918]\n",
      " [ 3.083152    3.0881685   1.09849623  8.08738122  9.07103474]]\n",
      "b:  [[1.46617297]\n",
      " [1.47280484]\n",
      " [1.57754748]\n",
      " [3.06529398]\n",
      " [2.87116567]]\n",
      "Loss:  0.2690027781871053\n",
      "epoch:  17     iteration:  0 / 200\n",
      "W:  [[ 0.98711446  1.98053379  2.98274025  3.98085884  5.98003466]\n",
      " [ 1.01457226  2.02201434  3.01951905  4.02164674  6.02257881]\n",
      " [ 3.03931817  4.05939803  5.05266537  6.05840619  2.06092106]\n",
      " [ 9.02583631  1.03903097 10.03460687 12.03837922  1.04003176]\n",
      " [ 3.05884335  3.08889475  1.07881869  8.08741037  9.09117411]]\n",
      "b:  [[1.43932262]\n",
      " [1.5031699 ]\n",
      " [1.65947704]\n",
      " [3.11913061]\n",
      " [2.99378098]]\n",
      "Loss:  0.29536310180824665\n",
      "epoch:  18     iteration:  0 / 200\n",
      "W:  [[ 0.97918045  1.98759354  2.98758883  3.97941279  5.98405066]\n",
      " [ 1.02354484  2.01403047  3.0140358   4.02328209  6.01803712]\n",
      " [ 3.06352754  4.03785634  5.03787071  6.0628186   2.04866688]\n",
      " [ 9.0417445   1.02487573 10.02488517 12.04127865  1.03197944]\n",
      " [ 3.09507495  3.05665558  1.05667708  8.09401396  9.07283458]]\n",
      "b:  [[1.41469936]\n",
      " [1.53101635]\n",
      " [1.73461097]\n",
      " [3.16850178]\n",
      " [3.106226  ]]\n",
      "Loss:  0.018759807667201707\n",
      "epoch:  19     iteration:  0 / 200\n",
      "W:  [[ 0.98304297  1.98336362  2.98357097  3.98866971  5.98373935]\n",
      " [ 1.01917671  2.01881409  3.01857959  4.01281343  6.01838918]\n",
      " [ 3.05174168  4.05076327  5.05013056  6.03457259  2.0496168 ]\n",
      " [ 9.03399991  1.03335699 10.03294123 12.02271795  1.03260363]\n",
      " [ 3.07743631  3.07597202  1.07502512  8.05174113  9.07425621]]\n",
      "b:  [[1.39108448]\n",
      " [1.55772243]\n",
      " [1.80666802]\n",
      " [3.2158511 ]\n",
      " [3.21406616]]\n",
      "Loss:  0.2062818076858212\n",
      "epoch:  20     iteration:  0 / 200\n",
      "W:  [[ 0.98444855  1.98540632  2.98112523  3.98728052  5.98062515]\n",
      " [ 1.01758714  2.016504    3.02134548  4.01438446  6.02191102]\n",
      " [ 3.04745278  4.04453029  5.05759335  6.03881146  2.05911926]\n",
      " [ 9.03118164  1.02926125 10.03784509 12.02550335  1.03884778]\n",
      " [ 3.07101756  3.06664378  1.08619387  8.05808501  9.08847754]]\n",
      "b:  [[1.36790847]\n",
      " [1.58393218]\n",
      " [1.87738592]\n",
      " [3.26232045]\n",
      " [3.31990218]]\n",
      "Loss:  0.5645862759967668\n",
      "epoch:  21     iteration:  0 / 200\n",
      "W:  [[ 0.98289729  1.9903782   2.98510155  3.99251278  5.98404113]\n",
      " [ 1.01934146  2.01088129  3.01684866  4.0084673   6.0180479 ]\n",
      " [ 3.0521862   4.02935938  5.04546024  6.02284605  2.04869596]\n",
      " [ 9.03429201  1.01929231 10.02987232 12.01501234  1.03199854]\n",
      " [ 3.07810157  3.04393909  1.06803554  8.03419128  9.0728781 ]]\n",
      "b:  [[1.34738284]\n",
      " [1.60714463]\n",
      " [1.94001661]\n",
      " [3.30347563]\n",
      " [3.41363491]]\n",
      "Loss:  0.009321546974469143\n",
      "epoch:  22     iteration:  0 / 200\n",
      "W:  [[ 0.98601369  1.9894938   2.98926094  3.98727144  5.99039379]\n",
      " [ 1.01581713  2.01188146  3.01214481  4.01439474  6.01086366]\n",
      " [ 3.04267701  4.03205798  5.03276854  6.03883918  2.02931181]\n",
      " [ 9.02804344  1.02106558 10.02153249 12.02552157  1.01926105]\n",
      " [ 3.06387017  3.04797779  1.04904121  8.0581265   9.04386789]]\n",
      "b:  [[1.32759278]\n",
      " [1.62952521]\n",
      " [2.00040282]\n",
      " [3.34315593]\n",
      " [3.50400856]]\n",
      "Loss:  0.09803117761197563\n",
      "epoch:  23     iteration:  0 / 200\n",
      "W:  [[ 0.98367066  1.98892234  2.98528787  3.98601989  5.98801739]\n",
      " [ 1.01846685  2.01252773  3.01663796  4.01581012  6.01355113]\n",
      " [ 3.04982637  4.0338017   5.04489173  6.04265809  2.03656302]\n",
      " [ 9.03274135  1.0222114  10.02949875 12.02803101  1.02402588]\n",
      " [ 3.07456987  3.05058744  1.06718471  8.06384186  9.05472   ]]\n",
      "b:  [[1.30790392]\n",
      " [1.65179136]\n",
      " [2.06048027]\n",
      " [3.38263335]\n",
      " [3.59392012]]\n",
      "Loss:  0.02089204916929416\n",
      "epoch:  24     iteration:  0 / 200\n",
      "W:  [[ 0.99073079  1.9850138   2.98562026  3.98558629  5.987205  ]\n",
      " [ 1.01048256  2.0169479   3.01626205  4.01630047  6.01446987]\n",
      " [ 3.02828353  4.045728    5.04387748  6.04398115  2.03904189]\n",
      " [ 9.01858535  1.03004827 10.02883228 12.0289004   1.02565477]\n",
      " [ 3.04232896  3.06843627  1.06566678  8.06582193  9.05842988]]\n",
      "b:  [[1.29014948]\n",
      " [1.67186985]\n",
      " [2.11465509]\n",
      " [3.4182321 ]\n",
      " [3.67499786]]\n",
      "Loss:  0.3906530808382943\n",
      "epoch:  25     iteration:  0 / 200\n",
      "W:  [[ 0.9915751   1.99163258  2.99090262  3.99139698  5.99072051]\n",
      " [ 1.00952772  2.00946271  3.01028823  4.00972916  6.01049417]\n",
      " [ 3.02570724  4.02553184  5.02775921  6.02625074  2.02831487]\n",
      " [ 9.01689245  1.0167772  10.01824082 12.01724959  1.01860595]\n",
      " [ 3.03847331  3.03821081  1.04154427  8.03928671  9.04237588]]\n",
      "b:  [[1.27413082]\n",
      " [1.68998536]\n",
      " [2.16353349]\n",
      " [3.45035052]\n",
      " [3.74814899]]\n",
      "Loss:  0.02107442600411563\n",
      "epoch:  26     iteration:  0 / 200\n",
      "W:  [[ 0.98858285  1.98990755  2.98777309  3.99240144  5.98942779]\n",
      " [ 1.01291166  2.01141355  3.01382741  4.00859322  6.01195612]\n",
      " [ 3.03483763  4.03079549  5.03730846  6.0231858   2.03225942]\n",
      " [ 9.02289211  1.02023599 10.02451571 12.01523559  1.02119795]\n",
      " [ 3.0521378   3.04608836  1.05583562  8.03469974  9.04827927]]\n",
      "b:  [[1.25781892]\n",
      " [1.7084325 ]\n",
      " [2.21330666]\n",
      " [3.48305691]\n",
      " [3.82263924]]\n",
      "Loss:  0.15679047452820322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  27     iteration:  0 / 200\n",
      "W:  [[ 0.98973     1.99309827  2.98928455  3.99104937  5.98914154]\n",
      " [ 1.01161434  2.00780517  3.01211811  4.01012227  6.01227984]\n",
      " [ 3.03133726  4.02105954  5.03269649  6.02731142  2.03313287]\n",
      " [ 9.02059199  1.01383841 10.02148515 12.01794657  1.0217719 ]\n",
      " [ 3.04689917  3.03151759  1.04893338  8.04087411  9.04958646]]\n",
      "b:  [[1.24274528]\n",
      " [1.72547928]\n",
      " [2.25930147]\n",
      " [3.5132805 ]\n",
      " [3.8914748 ]]\n",
      "Loss:  0.0773337923696236\n",
      "epoch:  28     iteration:  0 / 200\n",
      "W:  [[ 0.99076972  1.99021451  2.988856    3.98867202  5.99115997]\n",
      " [ 1.01043853  2.01106642  3.01260275  4.01281082  6.0099972 ]\n",
      " [ 3.02816474  4.02985888  5.03400414  6.03456553  2.02697395]\n",
      " [ 9.0185073   1.01962053 10.02234442 12.02271331  1.01772482]\n",
      " [ 3.04215119  3.04468663  1.05089041  8.05173057  9.04036906]]\n",
      "b:  [[1.22863364]\n",
      " [1.74143814]\n",
      " [2.30236089]\n",
      " [3.54157523]\n",
      " [3.95591729]]\n",
      "Loss:  0.22896726048867322\n",
      "epoch:  29     iteration:  0 / 200\n",
      "W:  [[ 0.98966968  1.98958735  2.99007159  3.99392785  5.99003024]\n",
      " [ 1.01168256  2.01177567  3.01122804  4.00686699  6.0112748 ]\n",
      " [ 3.03152132  4.03177254  5.03029496  6.01852819  2.03042113]\n",
      " [ 9.02071294  1.02087801 10.01990708 12.01217503  1.01998999]\n",
      " [ 3.04717463  3.0475506   1.04533926  8.02772918  9.04552809]]\n",
      "b:  [[1.21541295]\n",
      " [1.75638942]\n",
      " [2.3427017 ]\n",
      " [3.56808352]\n",
      " [4.01629111]]\n",
      "Loss:  0.0629202857744057\n",
      "epoch:  30     iteration:  0 / 200\n",
      "W:  [[ 0.99042605  1.99024427  2.99342481  3.99046473  5.98914412]\n",
      " [ 1.01082719  2.01103276  3.00743589  4.01078344  6.01227692]\n",
      " [ 3.02921339  4.02976805  5.02006315  6.02909536  2.03312499]\n",
      " [ 9.01919637  1.01956085 10.01318367 12.01911882  1.02176672]\n",
      " [ 3.04372059  3.0445507   1.0300264   8.04354395  9.04957467]]\n",
      "b:  [[1.20250993]\n",
      " [1.77098146]\n",
      " [2.38207323]\n",
      " [3.5939549 ]\n",
      " [4.07521432]]\n",
      "Loss:  0.10769181961438315\n",
      "epoch:  31     iteration:  0 / 200\n",
      "W:  [[ 0.98806595  1.99334347  2.99185395  3.99087898  5.99100194]\n",
      " [ 1.01349623  2.00752788  3.00921237  4.01031497  6.0101759 ]\n",
      " [ 3.03641486  4.02031136  5.02485638  6.02783135  2.02745613]\n",
      " [ 9.02392853  1.01334677 10.01633335 12.01828822  1.01804167]\n",
      " [ 3.05449828  3.03039786  1.03719992  8.04165224  9.04109069]]\n",
      "b:  [[1.19079092]\n",
      " [1.78423449]\n",
      " [2.41783191]\n",
      " [3.61745225]\n",
      " [4.12873056]]\n",
      "Loss:  0.029584930970161166\n",
      "epoch:  32     iteration:  0 / 200\n",
      "W:  [[ 0.99169802  1.99263503  2.99466089  3.99043525  5.99394501]\n",
      " [ 1.00938871  2.00832904  3.006038    4.01081678  6.00684759]\n",
      " [ 3.02533218  4.02247302  5.01629145  6.02918532  2.01847584]\n",
      " [ 9.01664599  1.01476722 10.01070526 12.01917793  1.01214064]\n",
      " [ 3.03791199  3.03363299  1.02438169  8.04367859  9.02765084]]\n",
      "b:  [[1.1798206 ]\n",
      " [1.79664082]\n",
      " [2.45130609]\n",
      " [3.63944842]\n",
      " [4.17882782]]\n",
      "Loss:  0.0345592287958252\n",
      "epoch:  33     iteration:  0 / 200\n",
      "W:  [[ 0.99312227  1.99330547  2.99163766  3.9947718   5.99048369]\n",
      " [ 1.00777802  2.00757084  3.00945697  4.00591257  6.01076199]\n",
      " [ 3.02098628  4.02042729  5.02551634  6.01595301  2.02903749]\n",
      " [ 9.01379027  1.01342295 10.01676701 12.01048286  1.01908079]\n",
      " [ 3.03140795  3.03057136  1.03818761  8.02387518  9.04345734]]\n",
      "b:  [[1.16924637]\n",
      " [1.80859922]\n",
      " [2.48357166]\n",
      " [3.66065041]\n",
      " [4.2271163 ]]\n",
      "Loss:  0.011810350878891145\n",
      "epoch:  34     iteration:  0 / 200\n",
      "W:  [[ 0.9916655   1.99401025  2.99285532  3.99374217  5.99357949]\n",
      " [ 1.00942549  2.00677381  3.00807992  4.00707698  6.00726096]\n",
      " [ 3.0254314   4.01827677  5.02180086  6.01909477  2.01959116]\n",
      " [ 9.0167112   1.01200983 10.01432554 12.01254734  1.01287352]\n",
      " [ 3.03806049  3.02735291  1.03262704  8.02857712  9.02932002]]\n",
      "b:  [[1.15927299]\n",
      " [1.81987812]\n",
      " [2.51400385]\n",
      " [3.68064767]\n",
      " [4.27266093]]\n",
      "Loss:  0.04360945020698097\n",
      "epoch:  35     iteration:  0 / 200\n",
      "W:  [[ 0.99351939  1.99646508  2.99429104  3.99208923  5.99371437]\n",
      " [ 1.00732892  2.00399764  3.00645626  4.00894629  6.00710842]\n",
      " [ 3.01977454  4.01078624  5.01741997  6.02413846  2.01917958]\n",
      " [ 9.01299402  1.00708774 10.01144682 12.01586159  1.01260307]\n",
      " [ 3.02959447  3.01614263  1.02607063  8.03612548  9.02870406]]\n",
      "b:  [[1.14996096]\n",
      " [1.8304091 ]\n",
      " [2.54241803]\n",
      " [3.69931888]\n",
      " [4.31518544]]\n",
      "Loss:  0.08839553280251713\n",
      "epoch:  36     iteration:  0 / 200\n",
      "W:  [[ 0.99434507  1.9939842   2.99476896  3.99554469  5.99356489]\n",
      " [ 1.00639516  2.00680327  3.00591578  4.00503852  6.00727747]\n",
      " [ 3.0172551   4.01835626  5.01596168  6.01359468  2.01963572]\n",
      " [ 9.01133848  1.01206206 10.01048856 12.00893318  1.01290281]\n",
      " [ 3.02582389  3.02747188  1.02388816  8.02034572  9.02938671]]\n",
      "b:  [[1.14158491]\n",
      " [1.83988157]\n",
      " [2.5679762 ]\n",
      " [3.71611338]\n",
      " [4.35343565]]\n",
      "Loss:  0.007079049225564733\n",
      "epoch:  37     iteration:  0 / 200\n",
      "W:  [[ 0.99449837  1.99637656  2.99532042  3.99405132  5.99439003]\n",
      " [ 1.00622179  2.00409775  3.00529214  4.00672737  6.00634431]\n",
      " [ 3.01678734  4.01105635  5.014279    6.01815146  2.01711792]\n",
      " [ 9.01103111  1.00726522 10.00938285 12.01192748  1.01124834]\n",
      " [ 3.02512384  3.01654687  1.02136986  8.02716537  9.02561858]]\n",
      "b:  [[1.13346965]\n",
      " [1.84905913]\n",
      " [2.59273863]\n",
      " [3.73238498]\n",
      " [4.39049495]]\n",
      "Loss:  0.0025572801277811146\n",
      "epoch:  38     iteration:  0 / 200\n",
      "W:  [[ 0.99507873  1.99585574  2.9946714   3.99432292  5.99389617]\n",
      " [ 1.00556547  2.00468674  3.00602612  4.00642021  6.00690282]\n",
      " [ 3.01501647  4.01264555  5.01625937  6.01732269  2.01862486]\n",
      " [ 9.00986746  1.0083095  10.01068418 12.01138289  1.01223856]\n",
      " [ 3.02247357  3.01892525  1.02433369  8.02592504  9.02787387]]\n",
      "b:  [[1.12572467]\n",
      " [1.85781793]\n",
      " [2.61637121]\n",
      " [3.74791416]\n",
      " [4.42586334]]\n",
      "Loss:  0.006202363713261567\n",
      "epoch:  39     iteration:  0 / 200\n",
      "W:  [[ 0.99388007  1.99661549  2.99509645  3.99477863  5.99449786]\n",
      " [ 1.00692103  2.00382754  3.00554542  4.00590485  6.00622237]\n",
      " [ 3.01867398  4.01032729  5.0149624   6.01593219  2.0167889 ]\n",
      " [ 9.01227084  1.00678616 10.00983192 12.01046918  1.01103214]\n",
      " [ 3.02794737  3.01545577  1.02239264  8.02384403  9.02512618]]\n",
      "b:  [[1.11825552]\n",
      " [1.86626479]\n",
      " [2.63916213]\n",
      " [3.76289027]\n",
      " [4.4599721 ]]\n",
      "Loss:  0.0058473615061827924\n",
      "epoch:  40     iteration:  0 / 200\n",
      "W:  [[ 0.99536792  1.99526058  2.99546014  3.99540354  5.9958344 ]\n",
      " [ 1.00523842  2.00535981  3.00513413  4.00519814  6.00471088]\n",
      " [ 3.01413406  4.01446158  5.01385267  6.01402536  2.01271066]\n",
      " [ 9.00928762  1.00950283 10.00910271 12.00921619  1.00835229]\n",
      " [ 3.02115296  3.02164313  1.02073182  8.02099027  9.01902271]]\n",
      "b:  [[1.11153673]\n",
      " [1.87386307]\n",
      " [2.65966344]\n",
      " [3.77636186]\n",
      " [4.49065424]]\n",
      "Loss:  0.0018910115256392363\n",
      "[[ 0.99514408  1.99568137  2.99647281  3.99553934  5.99535741]\n",
      " [ 1.00549156  2.00488394  3.0039889   4.00504457  6.00525031]\n",
      " [ 3.01481706  4.0131776   5.01076265  6.013611    2.01416613]\n",
      " [ 9.00973642  1.00865912 10.00707223 12.00894391  1.00930869]\n",
      " [ 3.02217512  3.01972152  1.01610732  8.02037015  9.02120095]] [[1.10497702]\n",
      " [1.88128145]\n",
      " [2.67967934]\n",
      " [3.78951448]\n",
      " [4.52060992]]\n"
     ]
    }
   ],
   "source": [
    "def training(x, y_true):\n",
    "    epoch = 40\n",
    "    learningRate = 0.01\n",
    "    batchSize  = 2 # 先设为1，后面看下设为其他是不是会存在矛盾\n",
    "    count = 0\n",
    "    loss = 100.0\n",
    "    ## init W and b\n",
    "    W = np.random.randn(5,5)\n",
    "    b = np.random.randn(5,1)\n",
    "    \n",
    "    while count < epoch :\n",
    "        ## prepare feed data\n",
    "        shuffle = np.random.permutation(np.arange(len(x.T)))  ## 打乱顺序\n",
    "        trainData = x.T[shuffle]\n",
    "        trainLabel = y_true.T[shuffle]\n",
    "        for n in range(round(len(x.T)/batchSize)):\n",
    "        #for n in range(1):\n",
    "            feedData_train = trainData[n*batchSize:(n+1)*batchSize].T\n",
    "            feedData_label = trainLabel[n*batchSize:(n+1)*batchSize].T\n",
    "            y_predict = forward(W, feedData_train, b)\n",
    "            #print(y_predict)\n",
    "            L = y_predict - feedData_label  ## 这个的顺序一定不等乱，其实这里很奇怪，为什么不是求loss的梯度呢？而是求一个一阶差？\n",
    "            #print(L)\n",
    "            dw, db = backward(L, W, feedData_train, b)\n",
    "            loss = loss_MSE(y_predict, feedData_label)\n",
    "            if n%200==0:\n",
    "                print('epoch: ', count+1, '    iteration: ', n, '/', int(len(x.T)/batchSize))\n",
    "                print('W: ', W)\n",
    "                print('b: ', b)\n",
    "                print('Loss: ', loss)\n",
    "            W -= learningRate*dw\n",
    "            b -= learningRate*db\n",
    "        count += 1\n",
    "    print(W, b)\n",
    "training(x, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
