{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### chan 2020/11/20\n",
    "### convolution forward and backward\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### convolution layer: forward calculate\n",
    "def conv_forward(x, kernel, b, padding=(0,0), strides=(1,1)):\n",
    "    '''\n",
    "    inputs:\n",
    "    x: input tensor, shape(batchSize, channels, height, weight)\n",
    "    kernel: shape(channels, numKernel, k1, k2)\n",
    "    x * kernel = (batchSize, channels, height, weight)*(channels, numKernel, k1, k2) = (bacthSize, numKernel, height, weight)\n",
    "    b: bias, shape(numKernel,)\n",
    "\n",
    "    padding: (x,y), shape(h,w) --> shape(h+2x, w+2y)\n",
    "    strides: (x direction, y direction)\n",
    "    \n",
    "    output:\n",
    "    z: shape(batchSize, numKernel, 1+(height-k1)//strides[1], 1+(weight-k2)//strides[0])\n",
    "    '''\n",
    "    #assert kernel.shape[0]==x.shape[1]\n",
    "    #assert b.shape[0]==kernel.shape[1]\n",
    "    if padding != (0,0):\n",
    "        x = np.lib.pad(x, ((0,0),(0,0),(padding[0],padding[0]),(padding[1],padding[1])), 'constant', constant_values=0)\n",
    "        \n",
    "    ## if height/weight cant be divided by stride, then padding zero\n",
    "    channels, numKernel, k1, k2 = kernel.shape\n",
    "    while (x.shape[2]-k1)%strides[1] != 0 :\n",
    "        #print(x.shape[2], k1)\n",
    "        x = np.lib.pad(x, ((0,0),(0,0),(0,1),(0,0)), 'constant', constant_values=0)\n",
    "    while (x.shape[3]-k2)%strides[0] != 0 :\n",
    "        x = np.lib.pad(x, ((0,0),(0,0),(0,0),(0,1)), 'constant', constant_values=0)\n",
    "    assert x.shape[1] == channels\n",
    "    batchSize, channels, height, weight = x.shape\n",
    "    assert (height-k1)%strides[1] == 0 \n",
    "    assert (weight-k2)%strides[0] == 0 \n",
    "    #print(x.shape)\n",
    "\n",
    "    ## calculate conv\n",
    "    print(height-k1)\n",
    "    z = np.zeros((batchSize, numKernel, 1+(height-k1)//strides[1], 1+(weight-k2)//strides[0]))\n",
    "    for n in np.arange(batchSize):\n",
    "        for k in np.arange(numKernel):\n",
    "            for h in np.arange(height-k1+1)[::strides[1]]:\n",
    "                for w in np.arange(weight-k2+1)[::strides[0]]:\n",
    "                    #print(n,k,h,w)\n",
    "                    z[n, k, h//strides[1], w//strides[0]] = np.sum(x[n, :, h:h+k1, w:w+k2] * kernel[:,k]) + b[k]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "x = np.random.randn(2, 3, 20, 20).astype(np.float64)\n",
    "kernel = np.random.randn(3, 4, 2, 2).astype(np.float64) \n",
    "b = np.zeros(4).astype(np.float64)\n",
    "z = conv_forward(x, kernel, b, padding=(1,1), strides=(2,2))    \n",
    "print(z.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _insertZeros(dzNext, strides=(2,2)):\n",
    "    '''\n",
    "    For the dimention match, error_next should be transfomed \n",
    "    from (batchSize, numKernel, (H-k1)/strides[1]+1, (W-k2)/strides[0]+1  \n",
    "    to (batchSize, numKernel, H, W)\n",
    "    \n",
    "    here, this function insert 0 in dzNext, each row and col\n",
    "    for example, in dim 2 and 3, [[1,1],[2,2]] --> [[1, 0, 1],[0,0,0],[2,0,2]]\n",
    "    '''\n",
    "    _, _, H, W = dzNext.shape\n",
    "    if strides[1] > 1:\n",
    "        for row in np.arange(H-1, 0, -1):\n",
    "            for n in np.arange(strides[1]-1):\n",
    "                dzNext = np.insert(dzNext, row, 0, axis = 2)\n",
    "    if strides[0] > 1:\n",
    "        for col in np.arange(W-1, 0, -1):\n",
    "            for n in np.arange(strides[1]-1):\n",
    "                dzNext = np.insert(dzNext, col, 0, axis = 3)\n",
    "    return dzNext\n",
    "#pz = _insertZeros(z, (2,2))\n",
    "#print(pz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### convolution layer backward\n",
    "def conv_backward(dzNext, x, kernel, padding=(0,0), strides=(1,1)):\n",
    "    '''\n",
    "    inputs args:\n",
    "    dzNext: error term of current layer, defined as dLoss(y,y*)/dz, where z is the output of current layer\n",
    "    x: input of current layer, shape(batchSize, channels, height, weight)\n",
    "    kernel: convolution kernels current layer, shape(batchSize, numKernel, k1, k2)\n",
    "    \n",
    "    output args:\n",
    "    dK: d(dzNext)/d(weights in kernel)\n",
    "    db: d(dzNext)/d(bias in kernel)\n",
    "    dz: error term of layer before dzNext\n",
    "    '''\n",
    "    \n",
    "    assert kernel.shape[1] == dzNext.shape[1]    # because number of kernel equal to channel of output feature map\n",
    "    channels, numKernel, k1, k2 = kernel.shape\n",
    "    batchSize, numKernel, NH, NW = dzNext.shape  # NH = (H-k1)//strides[1] +1, NW too.\n",
    "    \n",
    "    ### calculate dz of current layer using dzNext, according to chain rule\n",
    "    ## dzNext: (batchSize, numKernel, H, W) -->(batchSize, numKernel, H, W)\n",
    "    dzNextTrans1 = _insertZeros(dzNext, strides=(2,2))\n",
    "    dzNextTrans2 = np.lib.pad(dzNextTrans1, \n",
    "                              ((0,0),(0,0),(k1-1,k1-1),(k2-1,k2-1)), 'constant', constant_values=0) # shape(batchSize, numKernel, H+1, W+1)\n",
    "    \n",
    "    ## kernel: (channels, numKernel, k1, k2) --> (numKernel, channels, k1, k2) and rot_180(kernel)\n",
    "    kernelTrans = np.flip(kernel, (2,3))\n",
    "    kernelTrans = np.swapaxes(kernelTrans, 0, 1)\n",
    "    \n",
    "    ## (1) set param b to zero.    (2) default strides is (1,1) and default padding is (0,0), so dz will be (batchSize, channels, H+1 -1, W+1 -1)\n",
    "    dz = conv_forward(dzNextTrans2.astype(np.float64), kernelTrans.astype(np.float64), np.zeros((numKernel,), dtype=np.float64))\n",
    "    \n",
    "    ### remove padding\n",
    "    if padding[0] > 0 and padding[1] > 0:\n",
    "        dz = dz[:, :, padding[0]:-padding[0], padding[1]:-padding[1]]\n",
    "    elif padding[0]>0:\n",
    "        dz = dz[:, :, padding[0]:-padding[0], :]\n",
    "    elif padding[1]>0:\n",
    "        dz = dz[:, :, :, padding[1]:-padding[1]]\n",
    "    \n",
    "    \n",
    "    ### calculate dk,  shape(channels, numKernel, k1, k2)\n",
    "    xTrans = np.swapaxes(x, 0, 1)\n",
    "    xTrans = np.lib.pad(xTrans,\n",
    "                       ((0,0),(0,0),(padding[0],padding[0]),(padding[1],padding[1])), 'constant', constant_values=0)\n",
    "    print(xTrans.shape)\n",
    "    print(dzNextTrans1.shape)\n",
    "    dk = conv_forward(xTrans.astype(np.float64), dzNextTrans1.astype(np.float64), np.zeros((numKernel,), dtype=np.float64))\n",
    "    \n",
    "    ### calculate db\n",
    "    db = np.sum(np.sum(np.sum(dzNext, axis=-1), axis=-1), axis=0)\n",
    "\n",
    "    return dk / batchSize, db / batchSize, dz\n",
    "\n",
    "#loss = np.mean(np.sum(np.square(z*0.01), axis=-1))  # 损失函数值\n",
    "#dzNext = z*0.01  # 损失函数关于网络输出的梯度\n",
    "#dK, db, dz = conv_backward(dzNext, x, kernel, padding=(1,1), strides=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4, 11, 11)\n",
      "[[[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      "  [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      "  [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      "  [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      "  [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      "  [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      "  [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]]]\n"
     ]
    }
   ],
   "source": [
    "# 没有padding,输入的高度和宽度是20*20,卷积核是2*2,输出高度和宽度就是20-2//2+1=11\n",
    "batchSize = 2\n",
    "channels = 3\n",
    "H = 20\n",
    "W = 20\n",
    "numKernel = 4\n",
    "k1 = 2\n",
    "k2 = 2\n",
    "padding_H = 1\n",
    "padding_W = 1\n",
    "strides_H = 2\n",
    "strides_W = 2\n",
    "x = np.random.randn(batchSize, channels, H, W).astype(np.float64)\n",
    "K = np.random.randn(channels, numKernel, k1, k2).astype(np.float64) \n",
    "b = np.zeros(numKernel).astype(np.float64)\n",
    "y_true = np.ones((batchSize, numKernel, (H+2*padding_H-k1)//strides_H +1, (W+2*padding_W-k2)//strides_W +1))\n",
    "print(y_true.shape)\n",
    "print(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:0,loss:15.117241302038604\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:1,loss:12.32401358071482\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:2,loss:10.0790961950495\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:3,loss:8.268717672828494\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:4,loss:6.80386640837687\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:5,loss:5.614687071758678\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:6,loss:4.646185527803582\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:7,loss:3.8549290446681215\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:8,loss:3.206505038887808\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:9,loss:2.6735591048189513\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:10,loss:2.234276376672\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:11,loss:1.8712029261665555\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:12,loss:1.5703285631719235\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:13,loss:1.3203710654602006\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:14,loss:1.112216002567759\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:15,loss:0.938477051055377\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:16,loss:0.7931498594891352\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:17,loss:0.6713387387576399\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:18,loss:0.5690401989418185\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:19,loss:0.48297098343632333\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:20,loss:0.41043103260734654\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:21,loss:0.3491939455845185\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:22,loss:0.29741915312731965\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:23,loss:0.25358128304879024\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:24,loss:0.21641318059613537\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:25,loss:0.1848598065185663\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:26,loss:0.15804082635687663\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:27,loss:0.13522016472588683\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:28,loss:0.11578115781197992\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:29,loss:0.09920621878419987\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:30,loss:0.08506015182492353\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:31,loss:0.07297642448777908\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:32,loss:0.06264584546161141\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:33,loss:0.0538072035732676\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:34,loss:0.046239510201786536\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:35,loss:0.0397555560148787\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:36,loss:0.03419654781809733\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:37,loss:0.029427635245498133\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:38,loss:0.025334172297920766\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:39,loss:0.02181858713754896\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:40,loss:0.01879775647804901\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:41,loss:0.016200799473056846\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:42,loss:0.013967221074222342\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:43,loss:0.012045347093768788\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:44,loss:0.01039100321346844\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:45,loss:0.00896639836804836\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:46,loss:0.007739179644173885\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:47,loss:0.006681631354582862\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:48,loss:0.005769995493865972\n",
      "20\n",
      "(2, 4, 11, 11)\n",
      "21\n",
      "(3, 2, 22, 22)\n",
      "(2, 4, 21, 21)\n",
      "1\n",
      "(3, 4, 2, 2)\n",
      "step:49,loss:0.0049838945373100315\n",
      "[[[[0.98984285 0.99513348 0.99387347 1.00149524 0.99133447 1.00395536\n",
      "    1.00787305 1.00348379 1.00468927 0.98852501 1.00433555]\n",
      "   [0.99945538 0.9736215  0.99091067 1.01897743 0.9980202  1.01846275\n",
      "    1.01662033 1.01241966 0.98702309 1.00282444 1.00003825]\n",
      "   [1.00447682 1.02859065 0.99988382 1.00734454 0.9846353  1.02896612\n",
      "    1.02394194 0.98252285 1.00088899 0.97721109 1.00939307]\n",
      "   [1.00318419 1.00738372 0.99479987 1.0093053  1.0085706  1.02123224\n",
      "    1.02945396 1.01331849 0.98784666 1.02200403 1.00833098]\n",
      "   [1.01344453 0.99226055 1.00942086 1.01776908 1.0145715  0.98930026\n",
      "    1.03531887 1.00289825 1.03432196 0.97988499 1.00961213]\n",
      "   [1.00357055 0.98459825 1.00398246 1.0009965  1.01891706 1.00300501\n",
      "    0.97431152 1.00146015 1.00318035 0.98671951 0.99464363]\n",
      "   [0.97829109 1.00723432 1.00286349 0.99639151 0.99967806 1.00927438\n",
      "    0.9866051  1.01636806 0.98059909 1.00046445 0.99479782]\n",
      "   [0.99118565 1.03006439 1.00605539 0.9878696  0.99253685 0.98828666\n",
      "    0.98438658 1.00321474 0.98429056 0.98282101 0.99477254]\n",
      "   [0.97024191 1.02539823 1.02059576 0.9921651  1.01703758 1.01456315\n",
      "    0.9829986  1.00912183 1.02023523 0.99383116 0.99414893]\n",
      "   [0.9898359  1.00854737 0.99912462 0.98098539 1.01784519 0.9576972\n",
      "    0.99430991 0.98668566 0.98849849 1.00061052 1.01289654]\n",
      "   [0.98934739 0.98588143 0.98544689 0.98659609 0.99907876 1.00902684\n",
      "    1.00863448 0.97839971 0.99548471 1.00482125 0.99496602]]\n",
      "\n",
      "  [[1.02234782 1.00513686 1.01133246 0.99248801 0.98976108 1.01509441\n",
      "    1.00329694 1.01373016 1.03031886 0.98502223 0.99952681]\n",
      "   [1.0121909  0.99174413 0.99697429 1.0267506  1.03139298 1.00491367\n",
      "    1.01551487 1.01571664 1.02826254 1.00182459 1.02249694]\n",
      "   [0.93137074 1.00856906 1.01159731 0.99220643 0.99731488 0.96510225\n",
      "    0.99131407 1.02696042 1.00853754 1.04951421 1.04053841]\n",
      "   [1.0022402  0.96793994 0.91444329 1.04489066 0.91392023 1.0056204\n",
      "    0.96745189 0.97158662 1.0201914  0.91811083 0.96633667]\n",
      "   [0.96688063 1.00398011 0.98606797 1.04003362 0.98719565 0.99807268\n",
      "    0.96706356 1.08864138 0.97089794 1.02442111 0.94947062]\n",
      "   [0.98798565 1.05270739 1.00637929 0.93864649 0.96877862 0.99843625\n",
      "    1.02703114 1.07721089 0.98985302 1.03179667 0.95206465]\n",
      "   [1.05185261 1.01825604 0.96175161 1.01418867 1.00172798 0.9957483\n",
      "    1.04064514 1.01118906 1.03695277 1.02175663 1.02397058]\n",
      "   [0.99927805 1.04586625 0.98922082 1.03269471 1.04409073 1.06420787\n",
      "    0.9389193  1.03452753 1.00359105 0.96855309 0.9773972 ]\n",
      "   [1.03017293 0.95551227 0.98728549 0.98845885 0.92790581 0.97431548\n",
      "    0.95919531 0.96806823 0.93289855 0.99120098 1.02493901]\n",
      "   [1.02667181 1.07984881 1.04342405 1.01450862 0.93737689 1.06456729\n",
      "    0.9500328  1.03926362 1.02070889 0.98433206 0.93823484]\n",
      "   [0.99933872 1.01513467 0.95900789 0.97205447 1.0056616  0.96563379\n",
      "    0.98353135 1.00021973 1.00557597 1.01585523 0.98544811]]\n",
      "\n",
      "  [[0.9803612  1.00152432 1.00429636 1.00856244 0.98779639 1.01178442\n",
      "    1.01089778 0.98444058 0.99516232 1.00467355 0.99572701]\n",
      "   [0.99071729 1.01759884 1.01090285 0.97597653 0.99456722 1.00024197\n",
      "    1.00340944 0.97037811 1.00725274 0.9877555  0.99764992]\n",
      "   [1.01813784 0.96242833 0.99695366 1.01352773 0.99595547 0.98833651\n",
      "    0.99435285 0.99837949 0.97426443 0.95980528 0.9892688 ]\n",
      "   [0.99177254 1.0228459  1.01460719 1.02411827 1.03841618 1.01641818\n",
      "    1.00889277 1.02047823 1.00121986 1.03352637 1.00888718]\n",
      "   [1.0294672  0.99187874 0.99098944 0.99980688 1.00563844 0.9890225\n",
      "    0.98265531 0.94738876 1.00375317 0.98321858 1.00521781]\n",
      "   [1.02002896 0.99246645 1.03105692 1.02979661 1.01753843 0.99695731\n",
      "    1.00812372 0.94568469 0.99096852 0.9711673  1.01914665]\n",
      "   [0.965246   0.99405718 1.00915978 0.97193628 1.01232744 1.00305317\n",
      "    1.001426   0.97836931 0.98169994 0.99756512 0.98436368]\n",
      "   [0.9965973  0.97764062 0.99452052 1.00468242 0.99170153 0.98775441\n",
      "    1.03736806 0.99404152 0.99862366 0.99336549 0.99722761]\n",
      "   [0.98879164 1.02854352 1.01011645 0.98521032 1.02521288 1.01552103\n",
      "    1.02982547 1.01578346 1.00933889 0.98720878 0.98730671]\n",
      "   [0.98653299 0.95019032 0.98053081 0.99329662 1.03224623 0.98941631\n",
      "    1.03022893 0.9798187  0.99237732 0.98814698 1.0224421 ]\n",
      "   [0.98845338 0.98106089 1.01680051 1.02455515 0.98585672 1.00369992\n",
      "    1.01401389 0.98989193 0.99866606 0.97937127 0.99398926]]\n",
      "\n",
      "  [[0.99833963 0.99848206 1.00367121 0.99873817 0.99623258 1.00988608\n",
      "    0.99908301 0.99572198 0.99246495 0.99846288 0.9926451 ]\n",
      "   [1.00104801 1.01229921 0.9903779  1.00726789 1.0082117  0.98513096\n",
      "    1.00899645 0.99274449 1.01405018 0.98544533 1.00535203]\n",
      "   [0.994747   0.98706682 0.98951329 0.98906464 0.99884849 0.99141929\n",
      "    0.99667413 1.01398462 0.9929213  0.99500877 1.00582714]\n",
      "   [0.99880666 0.99924493 0.96955605 1.01854526 0.98297397 0.99632414\n",
      "    0.98147029 0.98819873 1.01515725 0.9916359  0.98924761]\n",
      "   [0.99333996 0.99924659 0.99962102 0.99952413 0.98402799 0.9961136\n",
      "    0.97762028 1.0135305  0.98366571 1.00900381 0.97428769]\n",
      "   [0.99826646 1.00481092 1.00052862 0.98048902 0.9973566  0.99902325\n",
      "    0.99702215 1.00330735 1.00008004 0.99467901 0.98456892]\n",
      "   [0.99758007 1.01010587 0.97636754 0.99038821 1.00747163 1.01429334\n",
      "    0.99828616 0.99714753 0.99769117 1.01072433 1.01038494]\n",
      "   [1.00136167 1.01226311 0.99355652 1.01388217 1.0062095  1.01136056\n",
      "    1.00408623 0.99682101 0.97989574 0.98853006 0.99556732]\n",
      "   [1.00240681 1.00299753 1.00625683 0.98652774 0.98859257 0.98871564\n",
      "    0.99489579 1.00084018 0.98691844 0.99102775 1.00721373]\n",
      "   [0.99467081 1.00629235 0.99342902 1.01121733 0.99750861 1.00796967\n",
      "    1.00239618 1.00567968 0.99347934 0.99108567 0.98084419]\n",
      "   [0.99136661 0.98964767 0.99518674 0.9969044  0.99484024 0.9962383\n",
      "    1.00131848 0.99614694 1.01817642 0.99722488 0.99972894]]]\n",
      "\n",
      "\n",
      " [[[0.98448825 0.99477811 0.98364042 0.9767092  1.01313708 1.00024968\n",
      "    0.99590453 1.00822246 0.98400731 0.99282293 1.00148268]\n",
      "   [1.00618487 0.98926302 0.98763935 0.9867509  1.01670508 0.99530861\n",
      "    0.97192766 1.00606163 1.02250459 1.03799192 1.00334078]\n",
      "   [1.00465065 1.0009388  1.02979064 1.0241299  1.03694991 1.005335\n",
      "    1.01411855 0.9962745  0.98451311 1.01403325 1.00846275]\n",
      "   [1.00276948 1.01696831 0.99597745 1.01202377 0.97912655 1.00526098\n",
      "    0.988214   0.99332028 1.00760183 0.98667551 1.00006467]\n",
      "   [1.00599832 0.98543304 0.9588553  1.00081386 0.97542594 0.98690605\n",
      "    0.9957708  1.00854411 1.00161624 1.00925306 0.97156863]\n",
      "   [1.00956951 0.97644309 0.98218652 1.00671089 0.985483   1.0008049\n",
      "    1.00021276 0.99648605 1.01601972 1.00711779 0.99611266]\n",
      "   [0.99558604 1.01666011 0.9832511  0.9886445  0.98708008 1.00394113\n",
      "    0.9746261  1.01144483 0.98802975 1.00694529 0.99804657]\n",
      "   [1.00654531 0.9981548  1.01565316 1.00572451 0.98942972 1.00591129\n",
      "    1.00229778 0.99666494 0.96735501 0.97490753 1.0011849 ]\n",
      "   [1.00756806 0.99771318 1.01908593 0.99644757 1.00823825 0.98489753\n",
      "    0.97374932 1.00320006 1.0056431  0.99627076 0.98735104]\n",
      "   [0.9878952  0.98380237 1.01677039 1.01923762 1.00200876 1.00991003\n",
      "    0.99757396 0.97245348 0.99488997 1.00406547 0.9914211 ]\n",
      "   [0.99985472 0.99591405 1.00471112 1.00080352 1.00051808 1.0080979\n",
      "    0.99367228 1.01023558 1.00162011 1.02332631 0.99332763]]\n",
      "\n",
      "  [[0.98366933 0.96221378 0.98888949 0.99613939 1.00145029 0.98880849\n",
      "    0.98483867 1.00792937 1.0216621  1.02999187 1.01461306]\n",
      "   [0.93793048 0.99973883 1.01397696 1.00733498 1.0154771  0.95152494\n",
      "    1.0946815  0.93096445 0.9691139  0.94893363 0.99616952]\n",
      "   [0.95929304 1.00294217 0.92916211 0.99017986 0.95084899 0.9779536\n",
      "    0.98638793 0.99947418 0.98106131 1.02968338 0.98747468]\n",
      "   [1.03948275 0.98388391 0.97744695 0.95826591 1.044746   1.04023854\n",
      "    1.00788466 0.9620967  0.98840392 0.95598576 0.99537913]\n",
      "   [0.99455141 1.00796686 1.00222739 0.98060568 1.00674232 0.99631102\n",
      "    0.99267118 1.06511676 1.03694672 0.98214117 0.9744223 ]\n",
      "   [0.99983392 1.02134876 1.0429557  0.94769618 0.9795984  1.01095901\n",
      "    1.0097879  0.95004089 0.98951297 0.95380089 1.02940296]\n",
      "   [0.99347746 0.9855182  1.09080071 0.98392998 0.96258634 0.95094048\n",
      "    1.03564713 0.99085347 0.9938659  0.99710009 1.04053432]\n",
      "   [0.9764976  0.95592136 0.99081598 0.96372281 0.95313141 0.99010568\n",
      "    0.92336306 0.9421865  1.01801542 0.95383349 0.95918003]\n",
      "   [1.0245951  1.02820064 1.00507072 0.952034   0.9960558  1.04335849\n",
      "    0.92751834 1.0183994  0.99729065 0.97723538 1.01878069]\n",
      "   [1.03636232 0.93437239 0.9510794  0.97911566 1.01847742 1.05920626\n",
      "    0.99110674 1.02966212 1.0295398  1.0181358  0.97491005]\n",
      "   [0.98349601 1.01401049 1.0141766  1.01964127 0.98102396 0.99642734\n",
      "    0.96921111 0.97592495 1.01820994 1.00735099 1.03749456]]\n",
      "\n",
      "  [[0.99627226 1.01199845 1.01103013 1.01390503 1.0131062  1.01779608\n",
      "    1.0061683  0.98829187 0.99524258 0.9661324  0.98092888]\n",
      "   [1.0168541  0.97980518 0.99720857 1.00500926 1.00658072 0.99619733\n",
      "    0.9807155  1.01373328 1.00362159 0.98979321 0.99689368]\n",
      "   [1.01837122 0.98179365 1.02449468 1.01391735 1.00460409 1.01749347\n",
      "    1.00755562 0.9811663  1.01704671 0.97076737 0.99674285]\n",
      "   [0.9835044  1.00406917 0.99134432 1.01608223 0.95947447 1.00861423\n",
      "    0.98570081 1.02682313 1.00064269 0.99348602 1.00381949]\n",
      "   [0.99797211 0.99861627 1.00359386 0.99042587 0.97863311 1.00933911\n",
      "    0.97401638 0.98953094 0.98588072 1.02818769 1.03304102]\n",
      "   [0.9942437  0.99118423 0.96992039 1.00858303 1.0087074  1.00926691\n",
      "    0.99521602 1.01032218 1.00880748 1.0105433  0.98925943]\n",
      "   [0.99658071 1.02852695 0.95640504 0.9987328  1.01024781 1.0433037\n",
      "    1.0094249  1.01253928 1.01101005 0.99518635 0.98139449]\n",
      "   [1.00397572 1.00783363 1.00693433 1.02945962 0.99578641 0.98566359\n",
      "    1.03537675 1.01716171 0.98218604 0.99348486 1.00073763]\n",
      "   [0.97578745 1.00349785 1.00789668 1.02142254 0.9938088  0.98081275\n",
      "    1.03528048 0.98483759 1.00903604 1.01006881 0.99086532]\n",
      "   [0.97789688 1.00742246 1.02036989 1.0024866  0.97764897 0.95573092\n",
      "    0.98188566 0.99700692 0.96899925 1.02696764 1.01831442]\n",
      "   [1.01486171 0.98480007 0.996289   0.98359296 0.99693499 1.00083786\n",
      "    1.03923597 0.99727491 0.97787755 1.00211819 0.98698701]]\n",
      "\n",
      "  [[0.98983469 1.00304147 1.00289911 1.00741713 1.00312728 1.00553037\n",
      "    0.99300918 0.99249284 0.99718251 0.99573288 0.99952353]\n",
      "   [0.98752758 0.99861775 1.00538848 0.99280158 1.00394545 0.98945544\n",
      "    1.02446455 0.99183801 0.9843506  0.98995392 0.97767287]\n",
      "   [0.99664669 0.98779953 0.99165378 1.00840305 0.99875071 0.98421527\n",
      "    0.98341897 0.99730566 1.00792466 0.99136309 0.99674879]\n",
      "   [0.99552227 0.98831222 1.00598307 0.98975165 0.99400031 1.01111047\n",
      "    0.99349574 0.99922972 0.98647939 0.98554342 1.00205004]\n",
      "   [1.00047115 1.00969072 0.99320291 1.00667692 0.99267643 1.01075438\n",
      "    0.99058791 1.02318274 1.00415258 0.9975767  0.99346116]\n",
      "   [1.00037192 1.01480147 0.99622831 0.98642655 0.99515539 0.99660596\n",
      "    0.99695026 0.98885443 1.00547091 0.98863261 1.00207401]\n",
      "   [1.00614841 1.00618244 1.01967488 1.00810346 0.98400092 1.0077709\n",
      "    1.02638327 0.99230738 0.99200864 1.00793941 1.00683089]\n",
      "   [0.99838609 0.99159995 0.98760842 1.00245973 0.97826142 0.98618552\n",
      "    1.00115585 0.99991825 0.98855317 0.98396198 1.00170195]\n",
      "   [0.99854724 1.00538223 1.00379922 0.97572916 1.00121879 1.00902965\n",
      "    0.99591676 1.00737369 0.99078692 0.99926872 1.00983213]\n",
      "   [1.00016513 0.98555241 0.99733987 0.99924788 0.99250973 0.99706654\n",
      "    0.99219722 0.99645612 0.99706024 1.00857949 0.98724759]\n",
      "   [1.00279976 1.00095917 1.00454596 1.00254763 0.98424412 0.99120835\n",
      "    1.00919372 0.99353916 1.00363152 1.0030734  1.01424345]]]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    # 前向\n",
    "    y_predict = conv_forward(x, K, b, padding=(padding_H, padding_W), strides=(strides_H, strides_W))\n",
    "    print(y_predict.shape)\n",
    "    \n",
    "    # 反向\n",
    "    loss = np.mean(np.sum(np.square(y_predict-y_true),axis=-1))\n",
    "    #loss, dy = mean_squared_loss(next_z, y_true)\n",
    "    dy = y_predict - y_true\n",
    "    \n",
    "    dK, db, _ = conv_backward(dy, x, K, padding=(padding_H, padding_W), strides=(strides_H, strides_W))\n",
    "    # 更新梯度\n",
    "    print(dK.shape)\n",
    "    K -= 0.001 * dK\n",
    "    b -= 0.001 * db\n",
    "\n",
    "    # 打印损失\n",
    "    print(\"step:{},loss:{}\".format(i, loss))\n",
    "\n",
    "    if np.allclose(y_true, y_predict):\n",
    "        print(\"yes\")\n",
    "        break\n",
    "\n",
    "print(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "z=1\n",
    "print(-z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(7//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "14\n",
      "16\n",
      "18\n",
      "20\n",
      "22\n",
      "24\n",
      "26\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "height = 30\n",
    "k1 = 2\n",
    "strides = (2,2)\n",
    "for h in np.arange(height - k1 + 1)[::strides[0]]:\n",
    "    print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [0 0]\n",
      " [1 2]]\n"
     ]
    }
   ],
   "source": [
    "a=np.array([[1,2],[1,2]])\n",
    "b=np.array([[3,3],[3,3]])\n",
    "a = np.insert(a, 1, 0, axis = 0)\n",
    "#print(a*b)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "H =10\n",
    "for h in np.arange(H - 1, 0, -1):\n",
    "    print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
