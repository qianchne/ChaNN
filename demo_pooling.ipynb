{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### chan 2020/11/24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### maximun pooling\n",
    "def pooling_max_forward(x, poolKernel, strides=(2,2), padding=(0,0)):\n",
    "    '''\n",
    "    x: input feature map with shape(batchSize, channels, heights, weights)\n",
    "    poolingKernel: pooling kernel\n",
    "    \n",
    "    z: output feature map after maximun pooling\n",
    "    '''\n",
    "    \n",
    "    batchSize, channels, H, W = x.shape\n",
    "    if padding != (0,0):\n",
    "        x = np.lib.pad(x, ((0,0),(0,0),(padding[0],padding[0]),(padding[1],padding[1])), 'constant', constant_values=0)\n",
    "    \n",
    "    z_H = (H + 2*padding[0] - poolKernel[0]) // strides[0] + 1\n",
    "    z_W = (W + 2*padding[0] - poolKernel[0]) // strides[0] + 1\n",
    "    \n",
    "    z = np.zeros((batchSize, channels, z_H, z_W))\n",
    "    \n",
    "    for n in np.arange(batchSize):\n",
    "        for c in np.arange(channels):\n",
    "            for h in np.arange(z_H):\n",
    "                for w in np.arange(z_W):\n",
    "                    z[n, c, h, w] = np.max(x[n, c, strides[0]*h : strides[0]*h+poolKernel[0], strides[1]*w : strides[1]*w+poolKernel[1]])\n",
    "    \n",
    "    return z\n",
    "\n",
    "def pooling_max_backward(dzNext, x, poolKernel, strides=(2,2), padding=(0,0)):\n",
    "    '''\n",
    "    池化的反向传播，只是梯度的重新分配。比较容易理解，池化一般是不会有填零这一操作的。\n",
    "    dzNext: error term of current layer, defined as dLoss(y,y*)/dz, where z is the output of current layer\n",
    "    x: input feature map with shape(batchSize, channels, heights, weights)\n",
    "    '''\n",
    "    \n",
    "    batchSize, channels, H, W = x.shape\n",
    "    _, _, dzNext_H, dzNext_W = dzNext.shape\n",
    "    \n",
    "    ### padding zeros\n",
    "    x = np.lib.pad(x, ((0, 0), (0, 0), (padding[0], padding[0]), (padding[1], padding[1])), 'constant', constant_values=0)\n",
    "    dz = np.zeros_like(x)\n",
    "    \n",
    "    ### rearrange dzNext to dz, according to the position of maximum\n",
    "    for n in arange(batchSize):\n",
    "        for c in arange(channels):\n",
    "            for h in arange(dzNext_H):\n",
    "                for w in arange(dzNext_W):\n",
    "                    flatten_idx = np.argmax(x[n, c,\n",
    "                                                   strides[0]*h : strides[0]*h+poolKernel[0],\n",
    "                                                   strides[1]*w : strides[1]*w+poolKernel[1]])\n",
    "                    h_idx = strides[0]*i + flatten_idx // poolKernel[1]\n",
    "                    w_idx = strides[1]*j + flatten_idx % poolKernel[1]\n",
    "                    dz[n, c, h_idx, w_idx] += dzNext[n, c, dzNext_H, dzNext_W]\n",
    "    ### remove padding\n",
    "    if padding[0] > 0 and padding[1] > 0:\n",
    "        dz = dz[:, :, padding[0]:-padding[0], padding[1]:-padding[1]]\n",
    "    elif padding[0]>0:\n",
    "        dz = dz[:, :, padding[0]:-padding[0], :]\n",
    "    elif padding[1]>0:\n",
    "        dz = dz[:, :, :, padding[1]:-padding[1]]\n",
    "    return dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### average pooling\n",
    "def pooling_average_forward(x, poolKernel, strides=(2,2), padding=(0,0)):\n",
    "    '''\n",
    "    x: input feature map with shape(batchSize, channels, heights, weights)\n",
    "    poolingKernel: pooling kernel\n",
    "    \n",
    "    z: output feature map after average pooling\n",
    "    '''\n",
    "    \n",
    "    batchSize, channels, H, W = x.shape\n",
    "    if padding != (0,0):\n",
    "        x = np.lib.pad(x, ((0,0),(0,0),(padding[0],padding[0]),(padding[1],padding[1])), 'constant', constant_values=0)\n",
    "    \n",
    "    z_H = (H + 2*padding[0] - poolKernel[0]) // strides[0] + 1\n",
    "    z_W = (W + 2*padding[0] - poolKernel[0]) // strides[0] + 1\n",
    "    \n",
    "    z = np.zeros((batchSize, channels, z_H, z_W))\n",
    "    \n",
    "    for n in np.arange(batchSize):\n",
    "        for c in np.arange(channels):\n",
    "            for h in np.arange(z_H):\n",
    "                for w in np.arange(z_W):\n",
    "                    z[n, c, h, w] = np.mean(x[n, c, strides[0]*h:strides[0]*h+poolKernel[0], strides[1]*w:strides[1]*w+poolKernel[1]])\n",
    "    \n",
    "    return z\n",
    "\n",
    "def pooling_average_backward(dzNext, x, poolKernel, strides=(2,2), padding=(0,0)):\n",
    "    '''\n",
    "    the backpropagation of pooling is the rearranement of dzNext, it has no paraments to update.\n",
    "    And, there is no padding in pooling usually.\n",
    "    dzNext: error term of current layer, defined as dLoss(y,y*)/dz, where z is the output of current layer\n",
    "    x: input feature map with shape(batchSize, channels, heights, weights)\n",
    "    '''\n",
    "    \n",
    "    batchSize, channels, H, W = x.shape\n",
    "    _, _, dzNext_H, dzNext_W = dzNext.shape\n",
    "    \n",
    "    ### padding zeros\n",
    "    x = np.lib.pad(x, ((0, 0), (0, 0), (padding[0], padding[0]), (padding[1], padding[1])), 'constant', constant_values=0)\n",
    "    dz = np.zeros_like(x)\n",
    "    \n",
    "    ### rearrange dzNext to dz\n",
    "    for n in arange(batchSize):\n",
    "        for c in arange(channels):\n",
    "            for h in arange(dzNext_H):\n",
    "                for w in arange(dzNext_W):\n",
    "                    dz[n, c, strides[0]*h : strides[0]*h+poolKernel[0], strides[1]*w : strides[1]*w+poolKernel[1]] +=\n",
    "                    dzNext[n, c, dzNext_H, dzNext_W] / (poolKernel[0]*poolKernel[1])\n",
    "                    \n",
    "    ### remove padding\n",
    "    if padding[0] > 0 and padding[1] > 0:\n",
    "        dz = dz[:, :, padding[0]:-padding[0], padding[1]:-padding[1]]\n",
    "    elif padding[0]>0:\n",
    "        dz = dz[:, :, padding[0]:-padding[0], :]\n",
    "    elif padding[1]>0:\n",
    "        dz = dz[:, :, :, padding[1]:-padding[1]]\n",
    "        \n",
    "    return dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### global average pooling\n",
    "def pooling_globalAverage_forward(x):\n",
    "    '''\n",
    "    global pooling output a value each feature map\n",
    "    x: input feature map with shape(batchSize, channels, heights, weights)\n",
    "    z: output feature map wih shape(batchSize, channels)\n",
    "    '''\n",
    "    z = np.mean(np.mean(x, axis=-1), axis=-1)\n",
    "    return z\n",
    "\n",
    "def pooling_globalAverage_backward(dzNext, dz, x):\n",
    "    '''\n",
    "    dzNext: error term of current layer, defined as dLoss(y,y*)/dz, where z is the output of current layer\n",
    "    x: input feature map with shape(batchSize, channels, heights, weights)\n",
    "    \n",
    "    dz: error term of layer before dzNext\n",
    "    '''\n",
    "    batchSize, channels, H, W = x.shape\n",
    "    \n",
    "    dz = np.zeros_like(x)\n",
    "    \n",
    "    ### rearrange dzNext to dz\n",
    "    for n in arange(batchSize):\n",
    "        for c in arange(channels):\n",
    "            dz[n, c, :, :] = dzNext[n,c] / H*W\n",
    "    return dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1200)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def flatten_forward(z):\n",
    "    \"\"\"\n",
    "    将多维数组打平，前向传播\n",
    "    :param z: 多维数组,形状(N,d1,d2,..)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    N = z.shape[0]\n",
    "    return np.reshape(z, (N, -1))\n",
    "\n",
    "\n",
    "def flatten_backward(next_dz, z):\n",
    "    \"\"\"\n",
    "    打平层反向传播\n",
    "    :param next_dz:\n",
    "    :param z:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return np.reshape(next_dz, z.shape)\n",
    "\n",
    "x = np.random.randn(2, 3, 20, 20).astype(np.float64)\n",
    "print(flatten_forward(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
